{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2024-02-19T22:58:00.609720-08:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.13\n",
      "IPython version      : 8.18.1\n",
      "\n",
      "Compiler    : GCC 11.2.0\n",
      "OS          : Linux\n",
      "Release     : 5.15.133.1-microsoft-standard-WSL2\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 28\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../data/cafa5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry ID</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59837</th>\n",
       "      <td>Q75PL7</td>\n",
       "      <td>MPPNSVDKTNETEYLKDNHVDYEKLIAPQASPIKHKIVVMNVIRFS...</td>\n",
       "      <td>59837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12347</th>\n",
       "      <td>E9QA15</td>\n",
       "      <td>MDDFERRRELRRQKREEMRLEAERIAYQRNDDDEEEAARERRRRAR...</td>\n",
       "      <td>12347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62582</th>\n",
       "      <td>Q84TI3</td>\n",
       "      <td>MKRLRSSDDLDFCNDKNVDGEPPNSDRPASSSHRGFFSGNNRDRGE...</td>\n",
       "      <td>62582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47476</th>\n",
       "      <td>Q24060</td>\n",
       "      <td>MRYLCVFSLTLILCCLSIKAQSLNCTRLRENCRPCTRRLVDPINDL...</td>\n",
       "      <td>47476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40185</th>\n",
       "      <td>P84282</td>\n",
       "      <td>APECGREAHCGDDCQSQVVTRDFDDRTCPKLLCCSKDGWCGNTDAN...</td>\n",
       "      <td>40185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Entry ID                                           Sequence  Index\n",
       "59837   Q75PL7  MPPNSVDKTNETEYLKDNHVDYEKLIAPQASPIKHKIVVMNVIRFS...  59837\n",
       "12347   E9QA15  MDDFERRRELRRQKREEMRLEAERIAYQRNDDDEEEAARERRRRAR...  12347\n",
       "62582   Q84TI3  MKRLRSSDDLDFCNDKNVDGEPPNSDRPASSSHRGFFSGNNRDRGE...  62582\n",
       "47476   Q24060  MRYLCVFSLTLILCCLSIKAQSLNCTRLRENCRPCTRRLVDPINDL...  47476\n",
       "40185   P84282  APECGREAHCGDDCQSQVVTRDFDDRTCPKLLCCSKDGWCGNTDAN...  40185"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(data_path / \"top100_train_split.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Entry ID', 'Sequence', 'Index'],\n",
       "    num_rows: 70921\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(df, preserve_index=False)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Entry ID', 'Sequence', 'Index'],\n",
       "        num_rows: 53190\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Entry ID', 'Sequence', 'Index'],\n",
       "        num_rows: 17731\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.25, seed=0)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Entry ID': 'Q6U841',\n",
       " 'Sequence': 'MEIKDQGAQMEPLLPTRNDEEAVVDRGGTRSILKTHFEKEDLEGHRTLFIGVHVPLGGRKSHRRHRHRGHKHRKRDRERDSGLEDGRESPSFDTPSQRVQFILGTEDDDEEHIPHDLFTELDEICWREGEDAEWRETARWLKFEEDVEDGGERWSKPYVATLSLHSLFELRSCILNGTVLLDMHANTLEEIADMVLDQQVSSGQLNEDVRHRVHEALMKQHHHQNQKKLTNRIPIVRSFADIGKKQSEPNSMDKNAGQVVSPQSAPACVENKNDVSRENSTVDFSKGLGGQQKGHTSPCGMKQRHEKGPPHQQEREVDLHFMKKIPPGAEASNILVGELEFLDRTVVAFVRLSPAVLLQGLAEVPIPTRFLFILLGPLGKGQQYHEIGRSIATLMTDEVFHDVAYKAKDRNDLVSGIDEFLDQVTVLPPGEWDPSIRIEPPKNVPSQEKRKIPAVPNGTAAHGEAEPHGGHSGPELQRTGRIFGGLILDIKRKAPYFWSDFRDAFSLQCLASFLFLYCACMSPVITFGGLLGEATEGRISAIESLFGASMTGIAYSLFGGQPLTILGSTGPVLVFEKILFKFCKEYGLSYLSLRASIGLWTATLCIILVATDASSLVCYITRFTEEAFASLICIIFIYEALEKLFELSEAYPINMHNDLELLTQYSCNCVEPHNPSNGTLKEWRESNISASDIIWENLTVSECKSLHGEYVGRACGHDHPYVPDVLFWSVILFFSTVTLSATLKQFKTSRYFPTKVRSIVSDFAVFLTILCMVLIDYAIGIPSPKLQVPSVFKPTRDDRGWFVTPLGPNPWWTVIAAIIPALLCTILIFMDQQITAVIINRKEHKLKKGCGYHLDLLMVAVMLGVCSIMGLPWFVAATVLSITHVNSLKLESECSAPGEQPKFLGIREQRVTGLMIFILMGSSVFMTSILKFIPMPVLYGVFLYMGASSLKGIQFFDRIKLFWMPAKHQPDFIYLRHVPLRKVHLFTIIQMSCLGLLWIIKVSRAAIVFPMMVLALVFVRKLMDLLFTKRELSWLDDLMPESKKKKLEDAEKEEEQSMLAMEDEGTVQLPLEGHYRDDPSVINISDEMSKTALWRNLLITADNSKDKESSFPSKSSPS',\n",
       " 'Index': 59130}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from transformers import AutoTokenizer, EsmModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer input max length: 1024\n",
      "Tokenizer vocabulary size: 33\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/esm2_t6_8M_UR50D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"Tokenizer input max length:\", tokenizer.model_max_length)\n",
    "print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_seqs(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"Sequence\"],\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27599fb3d1584a1eb25b5324cfc2e05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53190 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99130e2e1e54ab18eab06fc0013bebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17731 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Entry ID', 'Sequence', 'Index', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 53190\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Entry ID', 'Sequence', 'Index', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 17731\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_seqs, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Entry ID', 'Sequence', 'Index', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 53190\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Entry ID', 'Sequence', 'Index', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 17731\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"Index\"]\n",
    ")\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53190"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"].num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, dataset, split=\"train\"):\n",
    "        self.data = dataset[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.num_rows\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ProteinDataset(tokenized_dataset)\n",
    "val_dataset = ProteinDataset(tokenized_dataset, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88652, 100)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# targets\n",
    "targets = np.load(data_path / \"train_bp_top100_targets.npy\")\n",
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    batch = tokenizer.pad(batch)\n",
    "    batch[\"targets\"] = torch.as_tensor(\n",
    "        targets[batch[\"Index\"].tolist()], dtype=torch.float\n",
    "    )\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Index': tensor([11003, 10542, 79444, 63882, 82731, 12103,  5993, 27324]), 'input_ids': tensor([[ 0, 20, 15,  ...,  1,  1,  1],\n",
       "        [ 0, 20, 14,  ...,  1,  1,  1],\n",
       "        [ 0, 20, 19,  ...,  7, 17,  2],\n",
       "        ...,\n",
       "        [ 0, 20, 13,  ...,  1,  1,  1],\n",
       "        [ 0, 20,  9,  ...,  1,  1,  1],\n",
       "        [ 0, 20,  5,  ...,  1,  1,  1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'targets': tensor([[1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
       "         0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
       "         1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏃Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🤖 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = EsmModel.from_pretrained(model_name)\n",
    "embedding_dim = llm.config.hidden_size\n",
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetunedESM(nn.Module):\n",
    "    def __init__(self, llm, dropout_p, embedding_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.llm = llm\n",
    "        self.dropout_p = dropout_p\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.pre_classifier = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def mean_pooling(self, token_embeddings, attention_mask):\n",
    "        \"\"\"Average the embedding of all amino acids in a sequence\"\"\"\n",
    "\n",
    "        # expand the mask\n",
    "        expanded_mask = (\n",
    "            attention_mask.unsqueeze(-1).expand(token_embeddings.shape).float()\n",
    "        )\n",
    "\n",
    "        # sum unmasked token embeddings\n",
    "        sum_embeddings = torch.sum(token_embeddings * expanded_mask, dim=1)\n",
    "\n",
    "        # number of unmasked tokens for each sequence\n",
    "        # set a min value to avoid divide by zero\n",
    "        num_tokens = torch.clamp(expanded_mask.sum(1), min=1e-9)\n",
    "\n",
    "        # divide\n",
    "        mean_embeddings = sum_embeddings / num_tokens\n",
    "        return mean_embeddings\n",
    "\n",
    "    def forward(self, batch):\n",
    "        input_ids, attention_mask = batch[\"input_ids\"], batch[\"attention_mask\"]\n",
    "\n",
    "        # per token representations from the last layer\n",
    "        token_embeddings = self.llm(\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        ).last_hidden_state\n",
    "\n",
    "        # average per token representations\n",
    "        mean_embeddings = self.mean_pooling(token_embeddings, attention_mask)\n",
    "\n",
    "        # https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py\n",
    "        mean_embeddings = self.pre_classifier(mean_embeddings)  # (bs, embedding_dim)\n",
    "        mean_embeddings = nn.ReLU()(mean_embeddings)\n",
    "        mean_embeddings = self.dropout(mean_embeddings)\n",
    "\n",
    "        logits = self.classifier(mean_embeddings)  # (bs, num_classes)\n",
    "        return logits\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def predict(self, batch):\n",
    "        self.eval()\n",
    "        y = self(batch)\n",
    "        return y.cpu().numpy()\n",
    "\n",
    "    def save(self, dp):\n",
    "        with open(Path(dp, \"args.json\"), \"w\") as fp:\n",
    "            contents = {\n",
    "                \"dropout_p\": self.dropout_p,\n",
    "                \"embedding_dim\": self.embedding_dim,\n",
    "                \"num_classes\": self.num_classes,\n",
    "            }\n",
    "            json.dump(contents, fp, indent=4, sort_keys=False)\n",
    "\n",
    "        torch.save(self.state_dict(), Path(dp) / \"model.pt\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, esm_model, args_fp, state_dict_fp):\n",
    "        with open(args_fp, \"r\") as fp:\n",
    "            kwargs = json.load(fp=fp)\n",
    "\n",
    "        llm = EsmModel.from_pretrained(esm_model)\n",
    "        model = cls(llm=llm, **kwargs)\n",
    "        model.load_state_dict(\n",
    "            torch.load(state_dict_fp, map_location=torch.device(\"cpu\"))\n",
    "        )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of FinetunedESM(\n",
      "  (llm): EsmModel(\n",
      "    (embeddings): EsmEmbeddings(\n",
      "      (word_embeddings): Embedding(33, 320, padding_idx=1)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (position_embeddings): Embedding(1026, 320, padding_idx=1)\n",
      "    )\n",
      "    (encoder): EsmEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x EsmLayer(\n",
      "          (attention): EsmAttention(\n",
      "            (self): EsmSelfAttention(\n",
      "              (query): Linear(in_features=320, out_features=320, bias=True)\n",
      "              (key): Linear(in_features=320, out_features=320, bias=True)\n",
      "              (value): Linear(in_features=320, out_features=320, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (rotary_embeddings): RotaryEmbedding()\n",
      "            )\n",
      "            (output): EsmSelfOutput(\n",
      "              (dense): Linear(in_features=320, out_features=320, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (intermediate): EsmIntermediate(\n",
      "            (dense): Linear(in_features=320, out_features=1280, bias=True)\n",
      "          )\n",
      "          (output): EsmOutput(\n",
      "            (dense): Linear(in_features=1280, out_features=320, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (pooler): EsmPooler(\n",
      "      (dense): Linear(in_features=320, out_features=320, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "    (contact_head): EsmContactPredictionHead(\n",
      "      (regression): Linear(in_features=120, out_features=1, bias=True)\n",
      "      (activation): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (pre_classifier): Linear(in_features=320, out_features=320, bias=True)\n",
      "  (classifier): Linear(in_features=320, out_features=100, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "num_classes = 100\n",
    "\n",
    "model = FinetunedESM(\n",
    "    llm=llm, dropout_p=0.1, embedding_dim=embedding_dim, num_classes=num_classes\n",
    ")\n",
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Trainable Parameters: 7974941\n"
     ]
    }
   ],
   "source": [
    "print(f\"# Trainable Parameters: {count_parameters(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm.embeddings.word_embeddings.weight: True\n",
      "llm.embeddings.position_embeddings.weight: True\n",
      "llm.encoder.layer.0.attention.self.query.weight: True\n",
      "llm.encoder.layer.0.attention.self.query.bias: True\n",
      "llm.encoder.layer.0.attention.self.key.weight: True\n",
      "llm.encoder.layer.0.attention.self.key.bias: True\n",
      "llm.encoder.layer.0.attention.self.value.weight: True\n",
      "llm.encoder.layer.0.attention.self.value.bias: True\n",
      "llm.encoder.layer.0.attention.output.dense.weight: True\n",
      "llm.encoder.layer.0.attention.output.dense.bias: True\n",
      "llm.encoder.layer.0.attention.LayerNorm.weight: True\n",
      "llm.encoder.layer.0.attention.LayerNorm.bias: True\n",
      "llm.encoder.layer.0.intermediate.dense.weight: True\n",
      "llm.encoder.layer.0.intermediate.dense.bias: True\n",
      "llm.encoder.layer.0.output.dense.weight: True\n",
      "llm.encoder.layer.0.output.dense.bias: True\n",
      "llm.encoder.layer.0.LayerNorm.weight: True\n",
      "llm.encoder.layer.0.LayerNorm.bias: True\n",
      "llm.encoder.layer.1.attention.self.query.weight: True\n",
      "llm.encoder.layer.1.attention.self.query.bias: True\n",
      "llm.encoder.layer.1.attention.self.key.weight: True\n",
      "llm.encoder.layer.1.attention.self.key.bias: True\n",
      "llm.encoder.layer.1.attention.self.value.weight: True\n",
      "llm.encoder.layer.1.attention.self.value.bias: True\n",
      "llm.encoder.layer.1.attention.output.dense.weight: True\n",
      "llm.encoder.layer.1.attention.output.dense.bias: True\n",
      "llm.encoder.layer.1.attention.LayerNorm.weight: True\n",
      "llm.encoder.layer.1.attention.LayerNorm.bias: True\n",
      "llm.encoder.layer.1.intermediate.dense.weight: True\n",
      "llm.encoder.layer.1.intermediate.dense.bias: True\n",
      "llm.encoder.layer.1.output.dense.weight: True\n",
      "llm.encoder.layer.1.output.dense.bias: True\n",
      "llm.encoder.layer.1.LayerNorm.weight: True\n",
      "llm.encoder.layer.1.LayerNorm.bias: True\n",
      "llm.encoder.layer.2.attention.self.query.weight: True\n",
      "llm.encoder.layer.2.attention.self.query.bias: True\n",
      "llm.encoder.layer.2.attention.self.key.weight: True\n",
      "llm.encoder.layer.2.attention.self.key.bias: True\n",
      "llm.encoder.layer.2.attention.self.value.weight: True\n",
      "llm.encoder.layer.2.attention.self.value.bias: True\n",
      "llm.encoder.layer.2.attention.output.dense.weight: True\n",
      "llm.encoder.layer.2.attention.output.dense.bias: True\n",
      "llm.encoder.layer.2.attention.LayerNorm.weight: True\n",
      "llm.encoder.layer.2.attention.LayerNorm.bias: True\n",
      "llm.encoder.layer.2.intermediate.dense.weight: True\n",
      "llm.encoder.layer.2.intermediate.dense.bias: True\n",
      "llm.encoder.layer.2.output.dense.weight: True\n",
      "llm.encoder.layer.2.output.dense.bias: True\n",
      "llm.encoder.layer.2.LayerNorm.weight: True\n",
      "llm.encoder.layer.2.LayerNorm.bias: True\n",
      "llm.encoder.layer.3.attention.self.query.weight: True\n",
      "llm.encoder.layer.3.attention.self.query.bias: True\n",
      "llm.encoder.layer.3.attention.self.key.weight: True\n",
      "llm.encoder.layer.3.attention.self.key.bias: True\n",
      "llm.encoder.layer.3.attention.self.value.weight: True\n",
      "llm.encoder.layer.3.attention.self.value.bias: True\n",
      "llm.encoder.layer.3.attention.output.dense.weight: True\n",
      "llm.encoder.layer.3.attention.output.dense.bias: True\n",
      "llm.encoder.layer.3.attention.LayerNorm.weight: True\n",
      "llm.encoder.layer.3.attention.LayerNorm.bias: True\n",
      "llm.encoder.layer.3.intermediate.dense.weight: True\n",
      "llm.encoder.layer.3.intermediate.dense.bias: True\n",
      "llm.encoder.layer.3.output.dense.weight: True\n",
      "llm.encoder.layer.3.output.dense.bias: True\n",
      "llm.encoder.layer.3.LayerNorm.weight: True\n",
      "llm.encoder.layer.3.LayerNorm.bias: True\n",
      "llm.encoder.layer.4.attention.self.query.weight: True\n",
      "llm.encoder.layer.4.attention.self.query.bias: True\n",
      "llm.encoder.layer.4.attention.self.key.weight: True\n",
      "llm.encoder.layer.4.attention.self.key.bias: True\n",
      "llm.encoder.layer.4.attention.self.value.weight: True\n",
      "llm.encoder.layer.4.attention.self.value.bias: True\n",
      "llm.encoder.layer.4.attention.output.dense.weight: True\n",
      "llm.encoder.layer.4.attention.output.dense.bias: True\n",
      "llm.encoder.layer.4.attention.LayerNorm.weight: True\n",
      "llm.encoder.layer.4.attention.LayerNorm.bias: True\n",
      "llm.encoder.layer.4.intermediate.dense.weight: True\n",
      "llm.encoder.layer.4.intermediate.dense.bias: True\n",
      "llm.encoder.layer.4.output.dense.weight: True\n",
      "llm.encoder.layer.4.output.dense.bias: True\n",
      "llm.encoder.layer.4.LayerNorm.weight: True\n",
      "llm.encoder.layer.4.LayerNorm.bias: True\n",
      "llm.encoder.layer.5.attention.self.query.weight: True\n",
      "llm.encoder.layer.5.attention.self.query.bias: True\n",
      "llm.encoder.layer.5.attention.self.key.weight: True\n",
      "llm.encoder.layer.5.attention.self.key.bias: True\n",
      "llm.encoder.layer.5.attention.self.value.weight: True\n",
      "llm.encoder.layer.5.attention.self.value.bias: True\n",
      "llm.encoder.layer.5.attention.output.dense.weight: True\n",
      "llm.encoder.layer.5.attention.output.dense.bias: True\n",
      "llm.encoder.layer.5.attention.LayerNorm.weight: True\n",
      "llm.encoder.layer.5.attention.LayerNorm.bias: True\n",
      "llm.encoder.layer.5.intermediate.dense.weight: True\n",
      "llm.encoder.layer.5.intermediate.dense.bias: True\n",
      "llm.encoder.layer.5.output.dense.weight: True\n",
      "llm.encoder.layer.5.output.dense.bias: True\n",
      "llm.encoder.layer.5.LayerNorm.weight: True\n",
      "llm.encoder.layer.5.LayerNorm.bias: True\n",
      "llm.encoder.emb_layer_norm_after.weight: True\n",
      "llm.encoder.emb_layer_norm_after.bias: True\n",
      "llm.pooler.dense.weight: True\n",
      "llm.pooler.dense.bias: True\n",
      "llm.contact_head.regression.weight: True\n",
      "llm.contact_head.regression.bias: True\n",
      "pre_classifier.weight: True\n",
      "pre_classifier.bias: True\n",
      "classifier.weight: True\n",
      "classifier.bias: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import pytorch_lightning as pl\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from torchmetrics.functional.classification import multilabel_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESMLightningModule(L.LightningModule):\n",
    "    def __init__(self, model, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.model(batch)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logits = self(batch)\n",
    "        loss = self.loss_fn(logits, batch[\"targets\"])\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss  # this is passed to the optimizer for training\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits = self(batch)\n",
    "        loss = self.loss_fn(logits, batch[\"targets\"])\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "\n",
    "        f1_score = multilabel_f1_score(\n",
    "            logits, batch[\"targets\"].type(torch.int), num_classes\n",
    "        )\n",
    "        self.log(\"val_f1_score\", f1_score, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        logits = self(batch)\n",
    "\n",
    "        f1_score = multilabel_f1_score(\n",
    "            logits, batch[\"targets\"].type(torch.int), num_classes\n",
    "        )\n",
    "        self.log(\"f1_score\", f1_score, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_model = ESMLightningModule(model, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [ModelCheckpoint(save_top_k=1, mode=\"max\", monitor=\"val_f1_score\")]\n",
    "logger = CSVLogger(save_dir=\"logs/\", name=\"esm_model_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=3,\n",
    "    callbacks=callbacks,\n",
    "    accelerator=\"gpu\",\n",
    "    precision=\"16-mixed\",\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    deterministic=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type              | Params\n",
      "----------------------------------------------\n",
      "0 | model   | FinetunedESM      | 8.0 M \n",
      "1 | loss_fn | BCEWithLogitsLoss | 0     \n",
      "----------------------------------------------\n",
      "8.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.0 M     Total params\n",
      "31.900    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3461b78f44e74ebe8cd0d02d12fe9e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5b34f1c7d64a83b58961421f733f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756db189ba4e452f9620c9c6921f7620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9468ae90a2c5412aa6e0c810bc5adccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1956516eb2ad42a8a084329adbe19fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used: 8.69 GB\n",
      "Training Time: 48.76 min\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "trainer.fit(\n",
    "    model=lightning_model, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    ")\n",
    "end = time.time()\n",
    "trainer.print(f\"Memory used: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB\")\n",
    "print(f\"Training Time: {(end-start)/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
